# üìù Publications 

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2022</div><img src='images/cat-probing-emnlp22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure](https://arxiv.org/abs/2210.04633)

Nuo Chen<sup>&#42;</sup>, **Qiushi Sun**<sup>&#42;</sup>, Renyu Zhu<sup>&#42;</sup>, [Xiang Li](https://lixiang3776.github.io/), Xuesong Lu and Ming Gao

[Slides](./files/EMNLP22_Presentation.pdf) | [Video](https://youtu.be/lghmmlV62JA) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Revealing the relationship between the structural information of source codes and the attention distribution for CodePTMs, and exploring the impact of structural pre-training on task performance.
</div>
</div> -->

- `Preprint` [Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models](https://arxiv.org/abs/2311.09278), Fangzhi Xu, Zhiyong Wu, **Qiushi Sun**, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao and Jun Liu.
- `Preprint` [Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration](https://arxiv.org/abs/2310.00280), **Qiushi Sun**, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu and Lingpeng Kong.
- `Preprint` [Boosting Language Models Reasoning with Chain-of-Knowledge Prompting](https://arxiv.org/abs/2306.06427), Jianing Wang<sup>&#42;</sup>, **Qiushi Sun**<sup>&#42;</sup>, Nuo Chen, Xiang Li and Ming Gao.
- `Preprint` [TransCoder: Towards Unified Transferable Code Representation Learning Inspired by Human Skills](https://arxiv.org/abs/2306.07285), **Qiushi Sun**, Nuo Chen, Jianing Wang, Xiang Li and Ming Gao.
- `Preprint` [Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives](https://arxiv.org/abs/2305.08088), **Qiushi Sun**, Chengcheng Han, Nuo Chen, Renyu Zhu, Jingyang Gong, Xiang Li and Ming Gao.
- ``EMNLP 2023`` [Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding](https://arxiv.org/abs/2310.13022), Jianing Wang, **Qiushi Sun**, Nuo Chen, Chengyu Wang, Xiang Li, Ming Gao and Jun Huang.
- ``EMNLP 2023`` [Pass-Tuning: Towards Structure-Aware Parameter-Efficient Tuning for Code Representation Learning](qiushisun.github.io), Nuo Chen, **Qiushi Sun**, Jianing Wang, Xiang Li and Ming Gao.
- ``ACL 2023`` [Do Large Language Models Know What They Don't Know?](https://arxiv.org/abs/2305.18153), Zhangyue Yin, **Qiushi Sun**, Qipeng Guo, Jiawen Wu, Xipeng Qiu and Xuanjing Huang. [Slides](./files/ACL23_LLMSA-Presentation.pdf) \| [Video](https://youtu.be/VTl8FAdNJEI) \| [![](https://img.shields.io/github/stars/yinzhangyue/SelfAware?style=social&label=Code+Stars)](https://github.com/yinzhangyue/SelfAware)
- ``ACL 2023`` [When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario](https://arxiv.org/abs/2305.10013), Chengcheng Han, Liqing Cui, Renyu Zhu, Jianing Wang, Nuo Chen, **Qiushi Sun**, Xiang Li and Ming Gao.
- ``EMNLP 2022`` [CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure](https://arxiv.org/abs/2210.04633), Nuo Chen<sup>&#42;</sup>, **Qiushi Sun**<sup>&#42;</sup>, Renyu Zhu<sup>&#42;</sup>, Xiang Li, Xuesong Lu and Ming Gao. [Slides](./files/EMNLP22_Presentation.pdf) \| [Video](https://doi.org/10.48448/76ht-3y46) \| [![](https://img.shields.io/github/stars/QiushiSun/CodeAttention?style=social&label=Code+Stars)](https://github.com/QiushiSun/CodeAttention)
- ``CIKM 2023 (Demo)`` [HugNLP: A Unified and Comprehensive Library for Natural Language Processing](https://api.semanticscholar.org/CorpusID:257232389), Jianing Wang, Nuo Chen, **Qiushi Sun**, Wenkang Huang, Chengyu Wang and Ming Gao. \| <span style="color: #C41E3A;">**Best Demo Paper Award**</span> <img src='./images/hugnlp-logo.png' style='width: 2.95em;'> [![](https://img.shields.io/github/stars/wjn1996/HugNLP?style=social&label=Code+Stars)](https://github.com/HugAILab/HugNLP) 
- üòú More working drafts / preprints under review will be released later ‚åõÔ∏è