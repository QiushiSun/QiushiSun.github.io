# üìù Selected Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/science_board.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows](https://arxiv.org/abs/2505.19897) ü§ñüî¨

**Qiushi Sun**, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu, Kanzhi Cheng, Zhaoyang Liu, Jianing Wang, Qintong Li, Xiangru Tang, Tianbao Xie, Xiachong Feng, Xiang Li, Ben Kao, Wenhai Wang, Biqing Qi, Lingpeng Kong, Zhiyong Wu

[[Paper]](https://arxiv.org/abs/2505.19897) \| [[Slides]](./files/ScienceBoard_slides.pdf) \| [[Project]](https://qiushisun.github.io/ScienceBoard-Home/) \| [<img src='./images/svgs/huggingface_logo.svg' style='width: 1.35em;'>](https://huggingface.co/collections/OS-Copilot/os-genesis-6768d4b6fffc431dbf624c2d) [[Env]](https://huggingface.co/OS-Copilot/ScienceBoard-Env) \| [[Code]](https://github.com/OS-Copilot/ScienceBoard) \| [![](https://img.shields.io/github/stars/OS-Copilot/ScienceBoard?style=social&label=Code+Stars)](https://github.com/OS-Copilot/ScienceBoard) [<img src='./images/Logo_of_Twitter.svg.png' style='width: 1.35em;'>](https://x.com/qiushi_sun/status/1927720338072486387)

- First to apply computer-using agents to assist scientific exploration üåå
- Dynamic environment & benchmark for realistic scientific workflows üåç
- Comprehensive evaluation of SOTA LLM/VLM agents üß≠
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/os-sentinel-concise-overview.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows](https://arxiv.org/abs/2510.24411) üõ°Ô∏èüßê

**Qiushi Sun<sup>&#42;</sup>**, Mukai Li<sup>&#42;</sup>, Zhoumianze Liu<sup>&#42;</sup>, Zhihui Xie<sup>&#42;</sup>, Fangzhi Xu, Zhangyue Yin, Kanzhi Cheng, Zehao Li, Zichen Ding, Qi Liu, Zhiyong Wu, Zhuosheng Zhang, Ben Kao, Lingpeng Kong

[[Paper]](https://arxiv.org/abs/2510.24411) \| [Slides] \| [[Project]](https://qiushisun.github.io/OS-Sentinel-Home/) \| [<img src='./images/svgs/huggingface_logo.svg' style='width: 1.35em;'>](https://huggingface.co/collections/OS-Copilot/os-genesis-6768d4b6fffc431dbf624c2d) [[Env]](https://huggingface.co/OS-Copilot/ScienceBoard-Env) \| [[Code]](https://github.com/OS-Copilot/OS-Sentinel) \| [![](https://img.shields.io/github/stars/OS-Copilot/OS-Sentinel?style=social&label=Code+Stars)](https://github.com/OS-Copilot/OS-Sentinel) [<img src='./images/Logo_of_Twitter.svg.png' style='width: 1.35em;'>](https://x.com/qiushi_sun/status/1927720338072486387)

- MobileRisk-Live & MobileRisk, a dynamic environment and benchmark for realistic mobile agent safety üì±
- OS-Sentinel, a hybrid detection framework combining formal verification with contextual judgment üõ°Ô∏è
- Advanced mobile agent safety at both the step-level and trajectory-level üß≠
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025</div><img src='images/OS-Genesis.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis](https://arxiv.org/abs/2412.19723) üî•üî•

**Qiushi Sun<sup>&#42;</sup>**, Kanzhi Cheng<sup>&#42;</sup>, Zichen Ding<sup>&#42;</sup>, Chuanyang Jin<sup>&#42;</sup>, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu

[[Paper]](https://arxiv.org/abs/2412.19723) \| [[Slides]](./files/ACL25_OS_Genesis.pdf) \| [[Project]](https://qiushisun.github.io/OS-Genesis-Home/) \| [<img src='./images/svgs/huggingface_logo.svg' style='width: 1.35em;'>](https://huggingface.co/collections/OS-Copilot/os-genesis-6768d4b6fffc431dbf624c2d) [[Models & Data]](https://huggingface.co/collections/OS-Copilot/os-genesis-6768d4b6fffc431dbf624c2d) \| [![](https://img.shields.io/github/stars/OS-Copilot/OS-Genesis?style=social&label=Code+Stars)](https://github.com/OS-Copilot/OS-Genesis) [<img src='./images/Logo_of_Twitter.svg.png' style='width: 1.35em;'>](https://x.com/qiushi_sun/status/1874807124515344599)

- Shift from task-driven to interaction-driven GUI data synthesis ü§ñ
- A manual-free pipeline for constructing diverse GUI agent trajectories üß¨
- Great performance on online mobile/web benchmarks üåü
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/JanusCoder-Overview.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538) <img src='./images/logos/internlm.webp' style='width: 1.35em;'>

**Qiushi Sun**, Jingyang Gong, Yang Liu, Qiaosheng Chen, Lei Li, Kai Chen, Qipeng Guo, Ben Kao, Fei Yuan

[[Paper]](https://arxiv.org/abs/2510.23538) \| [Slides] \| [[Project]](https://github.com/InternLM/JanusCoder) \| [<img src='./images/svgs/huggingface_logo.svg' style='width: 1.35em;'>](https://huggingface.co/collections/internlm/januscoder) \| [[Code]](https://github.com/InternLM/JanusCoder) \| [![](https://img.shields.io/github/stars/InternLM/JanusCoder?style=social&label=Code+Stars)](https://github.com/InternLM/JanusCoder)

- JanusCoder series: foundational models establishing a unified visual-programmatic interface. ‚öôÔ∏è
- A versatile data synthesis toolkit for multimodal code intelligence. üõ†Ô∏è
- Superior performance on diverse text- and vision-centric tasks. üß≠

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Survey</div><img src='images/codelms-tree-v20-broad.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond](https://arxiv.org/abs/2403.14734) üî•üî•

**Qiushi Sun**, Zhirui Chen, Fangzhi Xu, Chang Ma, Kanzhi Cheng, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, Pengcheng Yin, Qipeng Guo, Xipeng Qiu, Xiaoli Li, Fei Yuan, Lingpeng Kong, Xiang Li, Zhiyong Wu 

[[Paper]](https://arxiv.org/abs/2403.14734) \| [[Slides]](./files/NCI_Survey_Slides_V1.pdf) \| [[Project]](https://qiushisun.github.io/NCI-Survey-Homapage/)  \| [Video] \| [![](https://img.shields.io/github/stars/QiushiSun/Awesome-Code-Intelligence?style=social&label=Code+Stars)](https://github.com/QiushiSun/Awesome-Code-Intelligence) [<img src='./images/Logo_of_Twitter.svg.png' style='width: 1.35em;'>](https://twitter.com/qiushi_sun/status/1773252567637639185)

 <!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> [![](https://img.shields.io/github/stars/QiushiSun/NCISurvey?style=social&label=Stars)](https://github.com/QiushiSun/NCISurvey) -->

Let me walk you through the development of neural code intelligence:
- Follow LMs for code as a thread to trace the field's development üöÄ
- Explore cross-domain synergies and opportunities üå±
- Present a broad array of promising research avenues üí°
<!-- - Provide multiple curated reading lists [![](https://img.shields.io/github/stars/QiushiSun/NCISurvey?style=social&label=Code+Stars)](https://github.com/QiushiSun/NCISurvey) -->
</div>
</div>

- `Preprint` [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096), Zeyi Sun<sup>&#42;</sup>, Yuhang Cao<sup>&#42;</sup>, Jianze Liang<sup>&#42;</sup>, **Qiushi Sun**<sup>&#42;</sup>, Ziyu Liu<sup>&#42;</sup>, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, Jiaqi Wang.
- `Preprint` [OS-MAP: How Far Can Computer Use Agents Go in Breadth and Depth?](https://arxiv.org/abs/2406.11736), Xuetian Chen, Yinghao Chen, Xinfeng Yuan, Lu Chen, Yuekeng Li, Zhoujia Zhang, Yingqian Huang, Leyan Huang, Jiaqing Liang, Tianbao Xie, Zhiyong Wu, **Qiushi Sun**<sup>&#9993;</sup>, Biqing Qi<sup>&#9993;</sup> and Bowen Zhou.
- `DL4C @ NIPS'25` [CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback](https://www.arxiv.org/abs/2507.22080), **Qiushi Sun**<sup>&#42;</sup>, Jingyang Gong<sup>&#42;</sup>, Lei Li, Qipeng Guo and Fei Yuan.
- `ACL 2025` [Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models](https://arxiv.org/abs/2406.11736), Fangzhi Xu, **Qiushi Sun**, Kanzhi Cheng, Jun Liu and Zhiyong Wu.
- `ACL 2025` [Dynamic and Generalizable Process Reward Modeling](https://aclanthology.org/2025.acl-long.212/), Zhangyue Yin, **Qiushi Sun**, Zhiyuan Zeng, Qinyuan Cheng, Xipeng Qiu and Xuanjing Huang.
- `ICLR 2025 (Spotlight)` [OS-ATLAS: A Foundation Action Model For Generalist GUI Agents](https://arxiv.org/abs/2410.23218), Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, **Qiushi Sun**, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang and Yu Qiao.
<!-- - `NAACL 2025 (Oral)` [KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models](http://arxiv.org/abs/2402.02801), Fei Yuan, Chang Ma, Shuai Yuan, **Qiushi Sun** and Lei Li. -->
- `COLM 2024` [Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration](https://arxiv.org/abs/2310.00280), **Qiushi Sun**, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu and Lingpeng Kong. <span style="color:#ac530f">[LLMAgents @ ICLR 2024]</span> [Slides](./files/COLM24_Corex_Presentation.pdf)
- `ACL 2024` [Boosting Language Models Reasoning with Chain-of-Knowledge Prompting](https://arxiv.org/pdf/2306.06427v3), Jianing Wang<sup>&#42;</sup>, **Qiushi Sun**<sup>&#42;</sup>, Xiang Li and Ming Gao. [Slides](./files/ACL24_CoK_Presentation.pdf)
- `ACL 2024` [SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents](https://arxiv.org/abs/2401.10935), Kanzhi Cheng, **Qiushi Sun**, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang and Zhiyong Wu. <span style="color:#ac530f">[LLMAgents @ ICLR 2024]</span>
- `ACL 2024` [Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models](https://arxiv.org/abs/2311.09278), Fangzhi Xu, Zhiyong Wu, **Qiushi Sun**, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao and Jun Liu.
- `COLING 2024` [TransCoder: Towards Unified Transferable Code Representation Learning Inspired by Human Skills](https://arxiv.org/abs/2306.07285), **Qiushi Sun**, Nuo Chen, Jianing Wang, Xiang Li and Ming Gao.
- `COLING 2024` [Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives](https://arxiv.org/abs/2305.08088), **Qiushi Sun**, Chengcheng Han, Nuo Chen, Renyu Zhu, Jingyang Gong, Xiang Li and Ming Gao. \| ü•à <span style="color: #C41E3A;"> **100K RMB Award-winning Solution** </span>
<!-- - `COLING 2024` [Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models](qiushisun.github.io), Zhangyue Yin, **Qiushi Sun**, Qipeng Guo, Zhiyuan Zeng, Xiaonan Li, Tianxiang Sun, Cheng Chang, Xipeng Qiu and Xuanjing Huang. -->
<!-- - `LREC-COLING 2024` [Structure-aware Fine-tuning for Code Pre-trained Models](qiushisun.github.io), Jiayi Wu, Renyu Zhu, **Qiushi Sun**, Nuo Chen, Xiang Li and Ming Gao. -->
- ``EMNLP 2023`` [Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication](http://arxiv.org/abs/2312.01823), Zhangyue Yin, **Qiushi Sun**, Cheng Chang, Qipeng Guo, Junqi Dai, Xuanjing Huang and Xipeng Qiu. [Slides](./files/EMNLP2023_EoT_231112.pdf) \| [Video](https://youtu.be/mAMn8Xjkprg?si=jDIj8oSy0oT0-WRp)
<!-- - ``EMNLP 2023`` [Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding](https://aclanthology.org/2023.findings-emnlp.528/), Jianing Wang, **Qiushi Sun**, Nuo Chen, Chengyu Wang, Xiang Li, Ming Gao and Jun Huang. -->
<!-- - ``EMNLP 2023`` [Pass-Tuning: Towards Structure-Aware Parameter-Efficient Tuning for Code Representation Learning](https://aclanthology.org/2023.findings-emnlp.42/), Nuo Chen, **Qiushi Sun**, Jianing Wang, Xiang Li and Ming Gao. -->
- ``CIKM 2023 (Demo)`` [HugNLP: A Unified and Comprehensive Library for Natural Language Processing](https://arxiv.org/abs/2302.14286), Jianing Wang, Nuo Chen, **Qiushi Sun**, Wenkang Huang, Chengyu Wang and Ming Gao. \| <span style="color: #C41E3A;"> üèÜ **Best Paper Award**</span> <img src='./images/hugnlp-logo.png' style='width: 3.15em;'> [![](https://img.shields.io/github/stars/wjn1996/HugNLP?style=social&label=Code+Stars)](https://github.com/HugAILab/HugNLP) 
- ``ACL 2023`` [Do Large Language Models Know What They Don't Know?](https://arxiv.org/abs/2305.18153), Zhangyue Yin, **Qiushi Sun**, Qipeng Guo, Jiawen Wu, Xipeng Qiu and Xuanjing Huang. [Slides](./files/ACL23_LLMSA-Presentation.pdf) \| [Video](https://youtu.be/VTl8FAdNJEI) \| [![](https://img.shields.io/github/stars/yinzhangyue/SelfAware?style=social&label=Code+Stars)](https://github.com/yinzhangyue/SelfAware)
<!-- - ``ACL 2023`` [When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario](https://arxiv.org/abs/2305.10013), Chengcheng Han, Liqing Cui, Renyu Zhu, Jianing Wang, Nuo Chen, **Qiushi Sun**, Xiang Li and Ming Gao. -->
- ``EMNLP 2022`` [CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure](https://arxiv.org/abs/2210.04633), Nuo Chen<sup>&#42;</sup>, **Qiushi Sun**<sup>&#42;</sup>, Renyu Zhu<sup>&#42;</sup>, Xiang Li, Xuesong Lu and Ming Gao. [Slides](./files/EMNLP22_Presentation.pdf) \| [Video](https://doi.org/10.48448/76ht-3y46) \| [![](https://img.shields.io/github/stars/QiushiSun/CodeAttention?style=social&label=Code+Stars)](https://github.com/QiushiSun/CodeAttention)

*Denotes equal contribution, &#9993; denotes corresponding author, more working drafts / preprints under review will be released later ‚åõÔ∏è